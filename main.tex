\documentclass[10pt]{article}

% Manage page layout
\usepackage[margin=2.5cm, includefoot, footskip=30pt]{geometry}
\pagestyle{plain}
\setlength{\parindent}{0em}
\setlength{\parskip}{1em}
\renewcommand{\baselinestretch}{1}

%%%%%%%PACKAGES HERE%%%%%%%
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{standalone}
\usepackage{booktabs}
\usepackage{setspace}
\usepackage[]{algorithm2e}
\usepackage[noend]{algpseudocode}
\usepackage{wrapfig}
\usepackage{hyperref}
\usepackage[toc,page]{appendix}
\usetikzlibrary{calc, shapes, patterns, decorations.pathreplacing}

\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother

\newcommand{\R}{\mathbb{R}}
\newtheorem{theorem}{Theorem}
\usetikzlibrary{decorations.pathmorphing, decorations.pathreplacing, angles,
                quotes, calc, er, positioning}

\newtheorem{lemma}[theorem]{Lemma}
\def\arraystretch{1.5}

\title{Stability of defection, optimisation of strategies and the limits of
       memory in the Prisoner's Dilemma.}
\author{Nikoleta E. Glynatsi \and Vincent A. Knight}
\date{}

\begin{document}

\maketitle

\begin{abstract}
    Memory-one strategies are a set of Iterated Prisoner's Dilemma strategies
    that have been praised for their mathematical tractability and performance
    against single opponents. This manuscript investigates \textit{best
    response} memory-one strategies as a multidimensional
    optimisation problem. Though extortionate memory-one strategies have gained
    much attention, we demonstrate that best response memory-one strategies do not
    behave in an extortionate way, and moreover, for memory one strategies to be
    evolutionary robust they need to be able to behave in a forgiving way. We
    also provide evidence that memory-one strategies suffer from their limited
    memory in multi agent interactions and can be out performed by
    longer memory strategies.
\end{abstract}

\section{Introduction}\label{section:introduction}

The Prisoner's Dilemma (PD) is a two player game used in understanding the
evolution of co-operative behaviour, formally introduced in~\cite{Flood1958}.
Each player has two options, to cooperate (C) or to defect (D). The decisions
are made simultaneously and independently. The normal form representation of the
game is given by:

\begin{equation}\label{equ:pd_definition}
    S_p =
    \begin{pmatrix}
        R & S  \\
        T & P
    \end{pmatrix}
    \quad
    S_q =
    \begin{pmatrix}
        R & T  \\
        S & P
    \end{pmatrix}
\end{equation}

where \(S_p\) represents the utilities of the row player and \(S_q\) the
utilities of the column player. The payoffs, \((R, P, S, T)\), are constrained
by equations~(\ref{eq:pd_constrain_one}) and~(\ref{eq:pd_constrain_two}).
Constraint~(\ref{eq:pd_constrain_one}) ensures that
defection dominates cooperation and constraint~(\ref{eq:pd_constrain_two})
ensures that there is a dilemma; the sum of the utilities for both players is
better when both choose to cooperate. The most common values used in the literature are
\((R, P, S, T) = (3, 1, 0, 5)\)~\cite{Axelrod1981}.


\begin{equation}\label{eq:pd_constrain_one}
    T > R > P > S
\end{equation}

\begin{equation}\label{eq:pd_constrain_two}
    2R > T + S
\end{equation}

The PD is a one shot game, however it is commonly studied in a manner where the
history of the interactions matters. The repeated form of the game is called the
Iterated Prisoner's Dilemma (IPD) and in the 1980s, following the work
of~\cite{Axelrod1980a, Axelrod1980b} it attracted the attention of the
scientific community. In~\cite{Axelrod1980a} and~\cite{Axelrod1980b}, the first
well known computer tournaments of the IPD were performed. A total of 13 and 63
strategies were submitted respectively in the form of computer code. The
contestants competed against each other, a copy of themselves and a random
strategy, and the winner was then decided on the average score achieved (not the
total number of wins). The contestants were given access to the entire history
of a match, however, how many turns of history a strategy would incorporate,
refereed to as the \textit{memory size} of a strategy, was a result of the
particular strategic decisions made by the author. The winning strategy of both
tournaments was the strategy called Tit for Tat and it's success, in both
tournaments, came as a surprise. Tit for Tat was a simple, forgiving strategy
that opened each interaction by cooperation, but it had managed to defeat far
more complicated opponents. Tit for Tat provided evidence that being nice can be
advantageous and became the major paradigm for reciprocal altruism.

Another trait of Tit for Tat is that it considers only the previous move of the
opponent. These type of strategies are called \textit{reactive} \cite{Nowak1989}
and are a subset of so called \textit{memory-one} strategies, which incorporate
both players' latests moves. Reactive and memory-one strategies have been
studied thoroughly in, for example~\cite{Nowak1990, Nowak1993}. They have gained
most of their attention when a certain subset of memory-one strategies was
introduced in~\cite{Press2012}. In~\cite{Stewart2012} it was stated that ``Press
and Dyson have fundamentally changed the viewpoint on the Prisoner's Dilemma''.

Zero-determinant strategies (ZD) are a special case of memory-one and
extortionate strategies. They chose their actions so that a linear relationship
is forced between their score and that of the opponent, ensuring that they will
always receive at least as much as their opponents. ZD strategies are indeed
mathematically unique and are proven to be robust in pairwise interactions.
Their true effectiveness in tournament interactions and evolutionary
dynamics has been questioned~\cite{adami2013, Knight2018, Harper2015}.

The purpose of this work is to consider a given memory-one strategy in a similar
fashion to~\cite{Press2012}, however whilst~\cite{Press2012} found a way for a
player to manipulate a given opponent, this work will consider a
multidimensional optimisation approach to identify the best response to a given
group of opponents. In particular, this work presents a compact method of
identifying the best response memory-one strategy against a given set of
opponents.

Further, theoretical and empirical results of this work include:

\begin{enumerate}
    \item The behaviour of a best response memory-one strategy and whether it
    behaves extortionately, similar to ~\cite{Press2012}.
    \item The factors that make a best response memory-one strategy evolutionary
    robust.
    \item A well designed framework that allows the comparison of an optimal
          memory one strategy, and a more complex strategy that has a larger
          memory and was obtained through contemporary reinforcement learning
          techniques~\cite{Harper2017}.
    \item An identification of conditions for which defection is known to be a
    best response; thus identifying environments where cooperation will not
    occur.
\end{enumerate}

The source code used in this manuscript has been written in a sustainable manner.
It is open source (\url{https://github.com/Nikoleta-v3/Memory-size-in-the-prisoners-dilemma})
and tested which ensures the validity of the results. It has also been archived
and can be found at.

\section{The utility}\label{section:utility}

One specific advantage of memory-one strategies is their mathematical
tractability. They can be represented completely as an element of \(\R^{4}_{[0, 1]}\). This
originates from~\cite{Nowak1989} where it is stated that if a strategy is
concerned with only the outcome of a single turn then there are four possible
`states' the strategy could be in; \(CC, CD, DC,CC\). Therefore, a memory-one
strategy can be denoted by the probability vector of cooperating after each of
these states; \(p=(p_1, p_2, p_3, p_4) \in \R_{[0,1]} ^ 4\). In an IPD match two
memory-one strategies are moving from state to state at each turn with a given
probability. This exact behaviour can be modeled as a stochastic process, and
more specifically as a Markov chain (Figure~\ref{fig:markov_chain}). The
corresponding transition matrix \(M\) of Figure~\ref{fig:markov_chain} is given
in (\ref{eq:transition_matrix}),

\begin{figure}
    \centering
    \includestandalone[width=.35\textwidth]{tex/markov_chain}
    \caption{Markov Chain}
    \label{fig:markov_chain}
\end{figure}

\begin{equation}\label{eq:transition_matrix}
    \input{tex/m_matrix.tex}
\end{equation}

The long run steady state probability vector \(v\) is the solution to \(v M = v\). The
stationary vector \(v\) can be combined with the payoff matrices of
(\ref{equ:pd_definition}) and the expected payoffs for each player
can be estimated without simulating the actual interactions. More
specifically, the utility for a memory-one strategy \(p\) against an opponent \(q\),
denoted as \(u_q(p)\), is defined by,

\begin{equation}\label{eq:press_dyson_utility}
    u_q(p) = v \cdot (R, S, T, P).
\end{equation}

The first theoretical result of this manuscript is presented in
Theorem~\ref{theorem:quadratic_form_u}. Theorem~\ref{theorem:quadratic_form_u}
states that \(u_q(p)\) is given by a ratio of two quadratic
forms~\cite{kepner2011}. To the authors knowledge our work is the fist to explore
the form of \(u_q(p)\).

\begin{theorem}\label{theorem:quadratic_form_u}
    The expected utility of a memory-one strategy \(p\in\mathbb{R}_{[0,1]}^4\)
    against a memory-one opponent \(q\in\mathbb{R}_{[0,1]}^4\), denoted
    as \(u_q(p)\), can be written as a ratio of two quadratic forms:

    \begin{equation}\label{eq:optimisation_quadratic}
    u_q(p) = \frac{\frac{1}{2}pQp^T + cp + a}
                {\frac{1}{2}p\bar{Q}p^T + \bar{c}p + \bar{a}},
    \end{equation}
    where \(Q, \bar{Q}\) \(\in \R^{4\times4}\) are square matrices whose
    diagonal elements are all equal to zero, and are defined by the
    transition probabilities of the opponent \(q_1, q_2, q_3, q_4\) as follows:

    \begin{center}
    \begin{equation}
    \resizebox{0.9\linewidth}{!}{\arraycolsep=2.5pt%
    \boldmath\(
    Q = \input{tex/q_numerator}\)},
    \end{equation}
    \begin{equation}\label{eq:q_bar_matrix}
    \resizebox{0.8\linewidth}{!}{\arraycolsep=2.5pt%
    \boldmath\(
    \bar{Q} =  \input{tex/q_denominator}\)}.
    \end{equation}
    \end{center}

    \(c \text{ and } \bar{c}\) \(\in \R^{4 \times 1}\) are similarly defined by:

    \begin{equation}\label{eq:q_matrix_numerator}
    \resizebox{0.3\linewidth}{!}{\arraycolsep=2.5pt%
    \boldmath\(c = \input{tex/c_numerator}\),}
    \end{equation}
    \begin{equation}\label{eq:q_matrix_denominator}
    \resizebox{0.3\linewidth}{!}{\arraycolsep=2.5pt%
    \boldmath\(\bar{c} = \input{tex/c_denominator}\).
    }
    \end{equation}
    and \(a = \input{tex/numerator_constant}\) and
    \(\bar{a} = \input{tex/denominator_constant}\).
\end{theorem}

The proof of Theorem~\ref{theorem:quadratic_form_u} is given in Appendix~\ref{section:appendix_a}.

Numerical simulations have been carried out to validate the formulation of
\(u_q(p)\) as a quadratic ratio. The simulated utility, which is denoted as
\(U_q(p)\), has been calculated using~\cite{axelrodproject} an open source
research framework for the study of the IPD~\footnote{The project is described
in~\cite{Knight2016}.}. For smoothing the simulated results the
simulated utility has been estimated in a tournament of 500 turns and 200
repetitions. Figure~\ref{fig:analytical_simulated} shows that the formulation
of Theorem~\ref{theorem:quadratic_form_u} successfully
captures the simulated behaviour.

\begin{figure}[!htbp]
    \begin{center}
        \begin{subfigure}{0.45\textwidth}
            \includegraphics[width=\linewidth]{img/validation_against_player_one.pdf}
        \end{subfigure}
        \begin{subfigure}{0.45\textwidth}
            \includegraphics[width=\linewidth]{img/validation_against_player_two.pdf}
        \end{subfigure}
    \end{center}
    \caption{Simulated and analytically calculated utility for \(p = (0, 1, 0, 1)\)
    and \(p = (0, \frac{2}{3}, \frac{1}{3}, 0)\) against \((\frac{1}{3}, \frac{1}{3}, \frac{1}{3}, q_4)\) for
    \(q_4 \in \{0,  \frac{1}{19}, \frac{2}{19}, \dots, \frac{18}{19}, 1\}\).}
    \label{fig:analytical_simulated}
\end{figure}

Theorem~\ref{theorem:quadratic_form_u} can be extended to consider multiple
opponents. The IPD is commonly studied in tournaments and/or Moran Processes
where a strategy interacts with a number of opponents. The payoff of a player in
such interactions is given by the average payoff the player received against
each opponent. More specifically the expected utility of a memory-one strategy
against a \(N\) number of opponents is given by
Theorem~\ref{theorem:tournament_utility}.

\begin{theorem}\label{theorem:tournament_utility}
    The expected utility of a memory-one strategy \(p\in\mathbb{R}_{[0,1]}^4\)
    against a group of opponents \(q^{(1)}, q^{(2)}, \dots, q^{(N)}\), denoted
    as \(\frac{1}{N} \sum\limits_{i=1} ^ {N} {u_q}^{(i)} (p)\), is given by:

    \begin{equation}\label{eq:tournament_utility}
        \frac{1}{N} \sum\limits_{i=1} ^ {N} {u_q}^{(i)} (p) = \frac{1}{N}
        \frac{\sum\limits_{i=1} ^ {N} (\frac{1}{2} pQ^{(i)} p^T + c^{(i)} p + a^ {(i)})
        \prod\limits_{\tiny\begin{array}{l} j=1 \\ j \neq i \end{array}} ^
        N (\frac{1}{2} p\bar{Q}^{(i)} p^T + \bar{c}^{(i)} p + \bar{a}^ {(i)})}
        {\prod\limits_{i=1} ^ N (\frac{1}{2} p\bar{Q}^{(i)} p^T + \bar{c}^{(i)} p + \bar{a}^ {(i)})}.
    \end{equation}
\end{theorem}

The proof of Theorem~\ref{theorem:tournament_utility} is a straightforward algebraic
manipulation.

Similar to the previous result, the formulation of Theorem~\ref{theorem:tournament_utility}
is validated using numerical simulations. The simulated and formulated utilities
of strategies in a tournament of 10 opponents as described in~\cite{Stewart2012}
are a match, Figure~\ref{fig:stewart_plotkin_results}.

\begin{figure}[!htbp]
    \begin{center}
    \includegraphics[width=.5\linewidth]{img/Stewart_tournament_results.pdf}
    \caption{The utilities of memory-one strategies \((\frac{1}{3}, \frac{1}{3}, \frac{1}{3}, p_4)\) for
    \(p_4 \in \{0,  \frac{1}{19}, \frac{2}{19}, \dots, \frac{18}{19}, 1\}\)
    against the 10 memory-one strategies used in~\cite{Stewart2012}.}
    \label{fig:stewart_plotkin_results}
    \end{center}
\end{figure}

Furthermore, using the list of strategies from~\cite{Stewart2012} we check
whether the utility against a group of strategies could be
captured by the utility against the mean opponent. Thus whether condition
\ref{eq:condition} holds. However, condition~(\ref{eq:condition}) fails as shown
in Figure~\ref{fig:hypothesis}.

\begin{equation}\label{eq:condition}
    \frac{1}{N} \sum_{i=1} ^ {N} {u_q}^{(i)} (p) = u_{\frac {1}{N} \sum\limits_{i=1} ^ N q^{(i)}}(p),
\end{equation}

\begin{figure}[!htbp]
    \begin{center}
    \includegraphics[width=.5\linewidth]{img/mean_vs_average_heatmap.pdf}
    \end{center}
    \caption{The difference between the average utility and against
    the utility against the average player of the strategies in~\cite{Stewart2012}.
    A positive difference indicates that the condition (\ref{eq:condition})
    does not hold.}
    \label{fig:hypothesis}
\end{figure}

Two theoretical results have been presented so far. The formulation of
Theorem~\ref{theorem:tournament_utility} which allows for the utility of a
memory-one strategy against any number of opponents to be estimated without
simulating the interactions is the main results used in this manuscript. In
Section~\ref{section:best_response_mem_one} it is used to  define best response
memory-one strategies and explore the conditions under which defection dominates
cooperation.

\section{Best responses to memory-one players}\label{section:best_response_mem_one}

This section focused on best responses and more specifically \textit{best
response memory-one} strategies. A \textit{best response} is the strategy which
corresponds to the most favorable outcome~\cite{Tadelis2013}, thus a best
response memory-one corresponds to a strategy \(p^*\) for which
(\ref{eq:tournament_utility}) is maximised. This is considered as a multi dimensional
optimisation problem given by,

\begin{equation}\label{eq:mo_tournament_optimisation}
    \begin{aligned}
    \max_p: & \ \sum_{i=1} ^ {N} {u_q}^{(i)} (p)
    \\
    \text{such that}: & \ p \in \R_{[0, 1]}
    \end{aligned}
\end{equation}

The decision variable is the vector \(p\), the solitary constraint is that \(p \in \R^4_{[0,
1]} \) and the objective function is (\ref{eq:tournament_utility}).

Optimising this particular ratio of quadratic forms is not trivial. It can be
verified empirically for the case of a single opponent that there exist at least
one point for which the definition of concavity does not hold. Some results are
known for non concave ratios of quadratic forms~\cite{Beck2009, Hongyan2014},
however, in these works it's assumed that either both the numerator and the
denominator of the fractional problem are concave or that the denominator is
greater than zero. Both assumptions fail here as stated in Theorem~\ref{theorem:concavity}.

\begin{theorem}\label{theorem:concavity}
    The utility of a player \(p\) against an opponent \(q\), \(u_q (p)\) given
    by (\ref{eq:optimisation_quadratic}), is not concave. Furthermore neither
    the numerator or the denominator of (\ref{eq:optimisation_quadratic}), are
    concave.
\end{theorem}

Proof is given in Appendix~\ref{appendix:proof_theorem_three}.

The non concavity of \(u(p)\) indicates multiple local optimal points. The
approach taken here is to introduce a compact way of constructing the candidate
set of all local optimal points. Once the set is defined the point that
maximises (\ref{eq:tournament_utility}) corresponds to the best response
strategy. The problem considered is bounded because \(p \in \R^4_{[0,
1]}\). Thus, the candidate solutions will exist either at the boundaries of the
feasible solution space, or within that space. The method of Lagrange
Multipliers~\cite{bertsekas2014} and Karush-Kuhn-Tucker
conditions~\cite{Giorgi2016} are based on this.

This approach allow us to define the best response memory-one strategy to a group of opponents,
Lemma~\ref{lemma:memone_group_best_response}.

\begin{lemma}\label{lemma:memone_group_best_response}

    The optimal behaviour of a memory-one strategy player
    \(p^* \in \R_{[0, 1]} ^ 4\)
    against a set of \(N\) opponents \(\{q^{(1)}, q^{(2)}, \dots, q^{(N)} \}\)
    for \(q^{(i)} \in \R_{[0, 1]} ^ 4\) is established by:

    \[p^* = \textnormal{argmax}\left(\sum\limits_{i=1} ^ N  u_q(p)\right), \ p \in S_q.\]

    The set \(S_q\) is defined as all the possible combinations of:

    \[
        S_q =
        \left\{p \in \mathbb{R} ^ 4 \left|
            \begin{aligned}
                \bullet\quad p_j \in \{0, 1\} & \quad \text{and} \quad \frac{d}{dp_k} 
                \sum\limits_{i=1} ^ N  u_q^{(i)}(p) = 0
                \quad \text{forall} \quad j \in J \quad \&  \quad k \in K  \quad \text{forall} \quad J, K \\
                & \quad \text{where} \quad J \cap K = \O \quad
                \text{and} \quad J \cup K = \{1, 2, 3, 4\}.\\
                \bullet\quad  p \in \{0, 1\} ^ 4
            \end{aligned}\right.
        \right\}.
    \]
\end{lemma}

The proof is given in the Appendix A.

Note that there is no immediate way to find the zeros of \(\frac{d}{dp} \sum\limits_{i=1} ^ N  u_q(p)\);

{\small
\begin{align}\label{eq:mo_tournament_derivative}
    \frac{d}{dp} \sum\limits_{i=1} ^ {N} {u_q}^{(i)} (p) & = \nonumber \\
    & =  \displaystyle\sum\limits_{i=1} ^ {N}
    \frac{\left(pQ^{(i)} + c^{(i)}\right) \left(\frac{1}{2} p\bar{Q}^{(i)} p^T + \bar{c}^{(i)} p + \bar{a}^ {(i)}\right)
    - \left(p\bar{Q}^{(i)} + \bar{c}^{(i)}\right) \left(\frac{1}{2} pQ^{(i)} p^T + c^{(i)} p + a^ {(i)}\right)}
    {\left(\frac{1}{2} p\bar{Q}^{(i)} p^T + \bar{c}^{(i)} p + \bar{a}^ {(i)}\right)^ 2}
\end{align}
}

For \(\frac{d}{dp} \sum\limits_{i=1} ^ N  u_q(p)\) to equal zero then:

{\scriptsize
\begin{align}\label{eq:polynomials_roots}
    \displaystyle\sum\limits_{i=1} ^ {N} \left(
    \left(pQ^{(i)} + c^{(i)}\right) \left(\frac{1}{2} p\bar{Q}^{(i)} p^T + \bar{c}^{(i)} p + \bar{a}^ {(i)}\right)
    - \left(p\bar{Q}^{(i)} + \bar{c}^{(i)}\right) \left(\frac{1}{2} pQ^{(i)} p^T + c^{(i)} p + a^ {(i)}\right)\right)
    &= 0, \quad {while} \\
    \displaystyle\sum\limits_{i=1} ^ {N} \frac{1}{2} p\bar{Q}^{(i)} p^T + \bar{c}^{(i)} p + \bar{a}^ {(i)} &\neq 0.
\end{align}}

Constructing the subset \(S_q\) is analytically possible. The points for any or
all of \(p_i \in \{0, 1\}\) for \(i \in \{1, 2, 3, 4\}\) are trivial. Finding the
roots of the partial derivatives \(\frac{d}{dp} \sum\limits_{i=1} ^ N  u_q(p)\)
which are a set of polynomials of equations (\ref{eq:polynomials_roots})
is feasible using resultant theory. Resultant theory~\cite{Jonsson2005} allow us
to solve systems of polynomials by the calculation of a resultant.

However, for large systems these quickly become intractable and numerical methods
taking advantage of the structure will be used which are described in
Section~\ref{section:numerical_experiments}. The rest of the section focuses
on an immediate theoretical result from Lemma~\ref{lemma:memone_group_best_response}.

\subsection{Stability of defection}\label{subsection:stability_of_defection}

An immediate result from Lemma~\ref{lemma:memone_group_best_response} can be
obtained by evaluating the sign of the derivative
(\ref{eq:mo_tournament_derivative}) at \(p=(0, 0, 0, 0)\). If at that point the
derivative is negative, then the utility of the player is maximum at that point
and it will only decrease if the player were to change their behaviour. Thus,
defection is the best response.

\begin{lemma}\label{lemma:stability_of_defection}
    In a tournament of \(N\) players where \(q^{(i)} = (q_{1}^{(i)}, q_{2}^{(i)}, q_{3}^{(i)}, q_{4}^{(i)})\)
    defection is a best response if the transition probabilities of the
    opponents satisfy the condition (\ref{eq:defection_condition}).

    \begin{equation}\label{eq:defection_condition}
        \sum_{i=1} ^ N (c^{(i)T} \bar{a}^{(i)} - \bar{c}^{(i)T} a^{(i)}) \leq 0
    \end{equation}

    while,

    \begin{equation}\label{eq:defection_condition}
        \sum_{i=1} ^ N \bar{a}^{(i)} \neq 0
    \end{equation}
\end{lemma}

\begin{proof}
    For defection to be a best response the derivative of the utility
    at the point \(p = (0, 0, 0, 0)\) must be negative. This would indicate that
    the utility function is only declining from that point onwards.

    Substituting \(p = (0, 0, 0, 0)\) in
    equation~(\ref{eq:mo_tournament_derivative}) gives:

    \begin{equation}
    \sum_{i=1} ^ N \frac{(c^{(i)T} \bar{a}^{(i)} - \bar{c}^{(i)T} a^{(i)})}
    {(\bar{a}^{(i)})^2}
    \end{equation}

    The sign of the numerator \( \displaystyle\sum_{i=1} ^ N (c^{(i)T} \bar{a}^{(i)} - \bar{c}^{(i)T} a^{(i)})\)
    can vary based on the transition probabilities of the opponents.
    The denominator can not be negative, and otherwise is always positive.
    Thus the sign of the derivative is negative if and only if
    \( \displaystyle\sum_{i=1} ^ N (c^{(i)T} \bar{a}^{(i)} - \bar{c}^{(i)T} a^{(i)}) \leq 0\).
\end{proof}

In an environment where defection is the best response the average payoff of
a defector is always higher than any other strategy can achieve. If we consider
a setting where in each the prevalence of each type of strategy was determined
by that strategy's success in the previous round, then in a population such that
(\ref{eq:defection_condition}) holds, defection would prevail;
thus cooperation would never occur.

This is demonstrated in Figures~\ref{fig:stable_defection} and~\ref{fig:unstable_defection}.

\begin{figure}[!htb]
    \centering
    \begin{minipage}{.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{img/population_defection_takes_over.pdf}
        \caption{For given opponents
        condition holds (\ref{eq:defection_condition}) and Defector takes over the population.}
        \label{fig:stable_defection}
    \end{minipage}\hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{img/population_defection_fails.pdf}
        \caption{For given opponents condition (\ref{eq:defection_condition}) fails and Defector does not take over the population.}
        \label{fig:unstable_defection}
    \end{minipage}
\end{figure}
\section{Numerical experiments} \label{section:numerical_experiments}

This section focuses on a series of numerical experiments in order to gain a
better understanding of memory-one strategies, their behaviour, robustness and
limitations. The problems that will be described in this section are solved
heuristically using Bayesian optimisation~\cite{Mokus1978}.

Bayesian optimisation is a global optimisation algorithm that has proven to
outperform many other popular algorithms~\cite{Jones2001}. The algorithm builds
a bayesian understanding of the objective function and it is well suited to the
multiple local optimas in the described search area of this work. Differential
evolution~\cite{Storn1997} was also considered, however it was not selected due
to Bayesian being computationally more efficient.

For example consider the problem of (\ref{eq:mo_tournament_optimisation}) where
\(N=2\). Figure~\ref{bayesian_example} illustrates the change of the utility
function over iterations of the algorithm. The default number of iterations that
has been used in this work is 60. After 60 calls the convergence of the utility
is checked. If the optimised utility has changed in the last 10\% iterations
then a further 20 iterations are considered.

\begin{figure}[!htbp]
    \begin{center}
    \includegraphics[width=.5\linewidth]{img/bayesian_example.pdf}
    \end{center}
    \caption{Utility over time of calls using Bayesian optimisation. The
    opponents are \(q^{(1)} = (\frac{1}{3}, \frac{1}{3}, \frac{1}{3},
    \frac{1}{3})\) and \(q^{(2)} = (\frac{1}{3}, \frac{1}{3},
    \frac{1}{3}, \frac{1}{3})\). The best response obtained is \(p^* = (0.0, \frac{11}{50}, 0.0, 0.0)\)}
    \label{bayesian_example}
\end{figure}

The rest of this section organized as follows. In
Section~\ref{subsection:best_response_n_2} is used to estimate a large number of
best response memory one strategies and explore whether they behave in an
extortionate way. In Section~\ref{subsection:best_respnse_evolutionary_setting}
similar, a large set of best responses are estimated by this time self
interactions are included. Finally in
Section~\ref{subsection:longer_memory_best_response}, for a large number of
opponents we compare the utility of best response memory one and longer memory
strategies.

\subsection{Best response memory-one strategies for \(N=2\)}\label{subsection:best_response_n_2}

As briefly discussed in Section~\ref{section:introduction}
zero-determinant strategies have been praised for their robustness against a
single opponent. By forcing a linear relationship between the scores
zero-determinant strategies can always receive a higher, or in the case of
mutual defection, the same payoff as their opponents. In IPD tournaments the
winner is decided on the average score a strategy received and not by wins. Thus
winning against an opponent does not guarantee a strategy's success.

We argue that by trying to exploit their opponents zero-determinant strategies
suffer in multi opponent interaction where the payoffs matter. In comparison,
best response memory-one strategies utilise their behaviour to gain the most
from their interactions. The aim of this section is to understand whether best
responses behave in an extortionate way, similarly to zero determinants. To
estimate a strategy's extortionate behaviour the SSE method as described in
[Knight 2019] is used. SSE is defined as the error, how far, a strategy is from
behaving extortionate. For example, a high SSE implies that a strategy is not
extortionate.

A large data set of best response memory-one strategies when \(N=2\) has been
generated and is available here. % ref The data set contains a total of 10000
trials corresponding to 10000 different best responses. For each trial a set of
2 opponents is randomly generated, the memory-one best response against them is
estimated. Though the probabilities \(q_i\) of the opponents are randomly
generated, Figures~\ref{fig:first_opponents_probabilities} and
\ref{fig:second_opponents_probabilities}, show that they are uniformly
distributed over the trial. Thus, the full space of possible opponents has been
covered.

\begin{figure}[!htbp]
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{img/first_opponent_probabilities.pdf}
        \subcaption{Distributions of first opponents' probabilities.}
        \label{fig:first_opponents_probabilities}
    \end{subfigure}
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{img/second_opponent_probabilities.pdf}
        \subcaption{Distributions of second opponents' probabilities.}
        \label{fig:second_opponents_probabilities}
    \end{subfigure}
\end{figure}

The SSE method has been applied to the data set and it's distribution is
shown in Figure~\ref{fig:sserror_mem_one} alongside a statistics summary in
Table~\ref{table:sserror_stats}.

\begin{figure}[!htbp]
    \begin{minipage}{0.72\textwidth}
            \begin{center}
                \includegraphics[width=\linewidth]{img/best_respones_sserror.pdf}
            \end{center}
                \caption{Distribution of SSE for memory-one best responses, when \(N=2\).}
                \label{fig:sserror_mem_one}
    \end{minipage}\hspace{1cm}
    \begin{minipage}{0.21\textwidth}
        \centering
        \captionsetup{type=table}
        \resizebox{.85\columnwidth}{!}{%
            \input{tex/sserror_table.tex}}
            \caption{Summary statistics SSE of best response memory one strategies included
            tournaments of \(N=2\).}
            \label{table:sserror_stats}
      \end{minipage}
\end{figure}

The distribution of SSE is skewed to left indicating that the best response does
exhibit extortionate behaviour, however, the best response is not uniformly
extortionate. A positive measure of skewness and kurtosis indicate a heavy tail
to the right, therefore, in several cases the strategy is not trying to
extortionate the opponent. To conclude, a best response strategy utilities it's
performance by behaving in a more adaptable way than zero-determinant
strategies. This analysis is extended to an evolutionary setting.

\subsection{Memory-one best responses in evolutionary dynamics}\label{subsection:best_respnse_evolutionary_setting}

The IPD is commonly studied in evolutionary processes where the strategies that
compose the population can adapt and change their behaviour based on the
outcomes of their interactions at each generation. In these processes self
interactions are key.

Self interactions can be incorporated in the formulation that has been used
in this paper. The utility of a memory-one strategy in an evolutionary setting
is given by,

\begin{equation}
    \frac{1}{N} \sum\limits_{i=1} ^ {N} {u_q}^{(i)} (p) + u_p(p).
\end{equation}

and respectively the optimisation problem of (\ref{eq:mo_tournament_optimisation})
is now re written as,

\begin{equation}\label{eq:mo_evolutionary_optimisation}
    \begin{aligned}
    \max_p: & \ \frac{1}{N} \sum\limits_{i=1} ^ {N} {u_q}^{(i)} (p) + u_p(p)
    \\
    \text{such that}: & \ p \in \R_{[0, 1]}
    \end{aligned}
\end{equation}

% Note that exact formulate are known for given evolutionary processes, however,
% for simplicity 17 is given to optimisation.

Solving this can done using \textit{best response dynamics} as detailed in
Algorithm~\ref{algo:best_response_dynamics}. Best response dynamics are commonly
used in evolutionary game theory. They represent a class of
strategy updating rules, where players strategies in the next round are
determined by their best responses to some subset of the population.


\begin{algorithm}[H]
    $p^{(t)}\leftarrow (1, 1, 1, 1)$\;
    \While{$p^{(t)} \neq p ^{(t -1)}$}{
     $p^{(t + 1)} =  \text{argmax} \frac{1}{N} \sum\limits_{i=1} ^ {N} {u_q}^{(i)}
     (p^{(t + 1)}) + u_p^{(t)}(p^{(t + 1)})$\;
    }
    \caption{Best response dynamics Algorithm}
    \label{algo:best_response_dynamics}
\end{algorithm}

Algorithm~\ref{algo:best_response_dynamics} starts by setting an initial solution \(p^{(1)}=(1, 1, 1, 1)\) and
repeatedly finds a strategy that maximises
(\ref{eq:mo_evolutionary_optimisation}) using the numerical approach described
in Section~\ref{section:numerical_experiments}. The algorithm stops when cycle
(a sequence of iterated evaluated points) is detected. A numerical example is
given in Figure~\ref{fig:best_response_dynamics_results}.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=.6\textwidth]{img/evolution_example_two.pdf}
    \caption{Best response dynamics with \(N=2\). More specifically, for
    \(q ^{(1)}=(0.2360,
                0.1031,
                0.3960,
                0.1549)\) and
    \(q ^{(2)}=(0.0665,
                0.4015,
                0.9179,
                0.8004)\).}
\label{fig:best_response_dynamics_results}
\end{figure}

For each of the 1000 pairs of opponents, from
Section~\ref{subsection:best_response_n_2}, the best response in an evolutionary
setting has also been obtained. The distribution of SSE for the best response
using evolutionary dynamics is given in Figure~\ref{fig:sserror_mem_one} and a
statistical summary of it's distribution in Table~\ref{table:sserror_stats}.

Similarly to the results of Section~\ref{subsection:best_response_n_2}, the
evolutionary best response strategy does not behave uniformly extortionate. A
larger value of both the kurtosis and the skewness of the SSE distribution
indicates that an evolutionary best response is more adaptable than the
equivalent best response. The difference between the strategies is further
explored. Figure~\ref{fig:behaviour_violin_plots} compares the tournament and
evolutionary best responses.
Table~\ref{table:wilcoxon_tests} details that no statistically significant
differences have been found. However, from
Figure~\ref{fig:behaviour_violin_plots}, it seems that evolutionary best
response is less likely to forgive after mutual defection whilst it is more
likely to forgive after being tricked (higher $p_2$ median).

\begin{figure}[!htbp]
    \begin{minipage}{0.72\textwidth}
            \begin{center}
            \includegraphics[width=\linewidth]{img/evo_sserror.pdf}
            \end{center}
            \caption{Distribution of SSE of best response memory-one strategies in
            evolutionary settings, when when \(N=2\).}
            \label{fig:sserror_mem_one}
    \end{minipage}\hspace{1cm}
    \begin{minipage}{0.21\textwidth}
        \centering
        \captionsetup{type=table}
        \resizebox{.85\columnwidth}{!}{%
            \input{tex/evo_sserror_table.tex}}
            \caption{Summary statistics SSE of best response memory-one strategies in
            evolutionary settings, when when \(N=2\).}
            \label{table:sserror_stats}
      \end{minipage}
\end{figure}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=.8\textwidth]{img/behaviour_violin_plots.pdf}
    \caption{Distributions of \(p^*\) for both best response and evo memory-one
    strategies.}
    \label{fig:behaviour_violin_plots}
\end{figure}

\begin{table}
    \centering
    \resizebox{.5\columnwidth}{!}{%
    \input{tex/medians_tests.tex}}
    \caption{A non parametric test, Wilcoxon Rank Sum, has been performed to
    tests the difference in the medians. A non parametric test is used because
    is evident that the data are skewed.}\label{table:wilcoxon_tests}
\end{table}

\subsection{Longer memory best response}\label{subsection:longer_memory_best_response}

The effectiveness of memory have been studied thoroughly in literature and
evidence where longer memories performed better and were more robustness. This
section focuses on proving that short memory strategies have limitations. More
specifically, we present several empirical results that show that longer
memories strategies can indeed perform better in cases of \(N=2\).

Similarly to Sections, a random number of 2 opponents is selceted and then the
best memeory one strategy is calculated, algosidned we caluclated train
to be etter.

This is achieved by comparing the performance of an optimised
memory-one strategy to that of a trained long memory-one.

The longer memory strategy selected is a strategy called \textit{Gambler},
introduced and discussed in~\cite{Harper2017}. A Gambler strategy makes
probabilistic decisions based on the opponent's \(n_1\) first moves, the
opponent's \(m_1\) last moves and the player's \(m_2\) last moves. This
manuscript considers $n_1 = 2, m_1 = 1$ and $m_2 = 1$. By considering the
opponent's first two moves, the opponents last move and the player's last move,
there are only 16 possible outcomes that can occur. Gambler also makes a
probabilistic decision of cooperating in the first move. Combining these Gambler
is a function \(f: \{\text{C, D}\}^{4 \cup 1} \rightarrow (0, 1)_{\R}\)

So this can be hard coded as an element of \([0, 1]_{\R} ^ {16 + 1}\) one
probability for each outcome plus the opening move.Thus in comparison to (),
finding and optimal Gambler is a 17 dimensional problem, which is solved
numerically using Bayesian optimisation. The optimisation problem is given by:

\begin{equation}\label{eq:gambler_optimisation}
    \begin{aligned}
    \max_p: & \ \sum_{i=1} ^ {N} {U_q}^{(i)} (F)
    \\
    \text{such that}: & \ F \in \R_{[0, 1]}^{17}
    \end{aligned}
\end{equation}

A graphical representation of Gambler and more specifically Gambler($n_1 = 2,
m_1 = 1, m_2 = 1$) is given by Figure~\ref{fig:gambler}.

\begin{figure}[!htbp]
    \centering
    \includestandalone{tex/gambler}
    \caption{Graphical representation of Gambler.}
    \label{fig:gambler}
\end{figure}

Bayesian optimisation is used to numerically solve
(\ref{eq:gambler_optimisation}). Similarly to the other experiments, two random
opponents are generated and the trained Gambler as well as the best response
memory-one are recorded for each trial. A total of 89 trials have been recorded.
The utility of both strategies for each trial is estimated,
Figure~\ref{fig:utilities_gambler_mem_one}.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=.55\textwidth]{img/gambler_performance_against_mem_one.pdf}
    \caption{Utilities of Gambler and best response memory-one strategies for
    89 different pair of opponents.}\label{fig:utilities_gambler_mem_one}
\end{figure}

Though Gambler has an infinite memory (in order to remember the opening moves of
the opponent) the information the strategy considers is not significantly larger
than memory-one strategies. Even so, it is evident from
Figure~\ref{fig:utilities_gambler_mem_one} that Gambler will always performs the
same or better than a best response memory one strategy, thus having a longer
memory is beneficial.

% \begin{figure}[!htbp]
%     \begin{minipage}{0.74\textwidth}
%             \begin{center}
%             \includegraphics[width=\linewidth]{img/gambler_sserror.pdf}
%             \end{center}
%             \caption{Distribution of sserrors for Gambler($n_1 = 2,
%             m_1 = 1, m_2 = 1$).}
%             \label{fig:sserror_gambler}
%     \end{minipage}
%     \begin{minipage}{0.24\textwidth}
%         \centering
%         \captionsetup{type=table}
%         \resizebox{.75\columnwidth}{!}{%
%             \input{tex/gambler_sserror_table.tex}}
%             \caption{Summary statistics SSerror}
%             \label{table:sserror_stats}
%       \end{minipage}
% \end{figure}

\section{Conclusion}

% This manuscript considers a specific set of IPD strategies called memory-one.
% Several scientific works have explored the memory-one set and have introduced a
% series of interesting strategies, most notable the ZD strategies. Compared to
% these works this paper explores the best response memory-one strategies. In the
% earlier sections of this paper, we have studied the analytical formulation of
% best response memory-one strategies and have presented a series of theorems and
% lemmas. This included the conditions for which cooperation will not emerge
% amongst memory-one strategies.

% The later part of the paper focused on a series of numerical experiments that
% aimed to provided insights into memory-one strategies. Best response and
% evolutionary best response strategies demonstrated insensitive of extortionate
% behaviour, however using the method described in [Knight2019], it was confirmed
% that neither are ZD. Thus, the strategies that can gain the highest payoff from
% their environment are not the ZD strategies. Moreover, to further explore the
% limitations of memory, and subsequently the limitations of ZD strategies, a
% strategy which considers more information from the history of play has been
% trained. Gambler outperformed the best response strategy in the smallest
% possible tournament of 3 strategies. In the future, we are aiming to consider
% the performance in larger tournaments.

% As stated~\cite{Press2012}, ZD strategies are indeed capable of dominating a
% single one opponent, however we raise the question; Is winning in your
% interactions the most outstanding feature of a strategy? In a repeated game,
% where a strategy's reputation and overall score from it's interactions matters,
% such as the Iterated Prisoner's Dilemma, we argue that is not.

\section{Acknowledgements}

A variety of software libraries have been used in this work:

\begin{itemize}
    \item The Axelrod library for IPD simulations~\cite{axelrodproject}.
    \item The Scikit-optimize library for an implementation of Bayesian optimisation~\cite{tim_head_2018_1207017}.
    \item The Matplotlib library for visualisation~\cite{hunter2007matplotlib}.
    \item The SymPy library for symbolic mathematics~\cite{sympy}.
    \item The Numpy library for data manipulation~\cite{walt2011numpy}.
\end{itemize}

% Bibliography
\bibliographystyle{plain}
\bibliography{bibliography.bib}

\begin{appendices}

\section{Proofs of the Theorems}\label{section:appendix_a}

\subsection{Proof of Theorem~\ref{theorem:quadratic_form_u}}
\input{tex/proof_theorem_one}

\subsection{Proof of Theorem~\ref{theorem:concavity}}\label{appendix:proof_theorem_three}
\input{tex/proof_theorem_three}

\end{appendices}

\end{document}

