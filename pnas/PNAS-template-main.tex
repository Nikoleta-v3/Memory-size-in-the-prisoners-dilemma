\documentclass[9pt,twocolumn,twoside,lineno]{pnas-new}
% Use the lineno option to display guide line numbers if required.

\templatetype{pnasbriefreport} % Choose template 
% {pnasresearcharticle} = Template for a two-column research article
% {pnasmathematics} %= Template for a one-column mathematics article
% {pnasinvited} %= Template for a PNAS invited submission
\usepackage{amsthm}
\newcommand{\R}{\mathbb{R}}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}

\title{Stability of defection, optimisation of strategies and the limits of
memory in the Prisoner's Dilemma.}

% Use letters for affiliations, numbers to show equal authorship (if applicable) and to indicate the corresponding author
\author[a, 1]{Nikoleta E. Glynatsi}
\author[a, 2]{Vince A. Knight}

\affil[a]{Cardiff University, School of Mathematics, Cardiff, United Kingdom}

% Please give the surname of the lead author for the running footer
\leadauthor{Nikoleta E. Glynatsi} 

% Please include corresponding author, author contribution and author declaration information
\authorcontributions{Please provide details of author contributions here.}
\authordeclaration{Please declare any competing interests here.}
\equalauthors{\textsuperscript{1}A.O.(Author One) contributed equally to this work with A.T. (Author Two) (remove if not applicable).}
\correspondingauthor{\textsuperscript{2}To whom correspondence should be addressed. E-mail: author.two\@email.com}

% At least three keywords are required at submission. Please provide three to five keywords, separated by the pipe symbol
\keywords{Prisoner's Dilemma $|$ zero-determinant $|$ best responses $|$ memory-one $|$ extortionate $|$}

\begin{abstract}
Memory-one strategies are a set of Iterated Prisoner's Dilemma strategies
that have been acclaimed for their mathematical tractability and performance
against single opponents. This manuscript investigates \textit{best
responses} to a collection of memory-one strategies as a multidimensional
optimisation problem. Though extortionate memory-one strategies have gained
much attention, we demonstrate that best response memory-one strategies do not
behave in an extortionate way, and moreover, for memory one strategies to be
evolutionary robust they need to be able to behave in a forgiving way. We
also provide evidence that memory-one strategies suffer from their limited
memory in multi agent interactions and can be out performed by
longer memory strategies.
\end{abstract}

\dates{This manuscript was compiled on \today}
\doi{\url{www.pnas.org/cgi/doi/10.1073/pnas.XXXXXXXXXX}}

\begin{document}

\maketitle
\thispagestyle{firststyle}
\ifthenelse{\boolean{shortarticle}}{\ifthenelse{\boolean{singlecolumn}}{\abscontentformatted}{\abscontent}}{}

% If your first paragraph (i.e. with the \dropcap) contains a list environment (quote, quotation, theorem, definition, enumerate, itemize...), the line after the list may have some extra indentation. If this is the case, add \parshape=0 to the end of the list environment.
\dropcap{T}he Prisoner's Dilemma (PD) is a two player game used in understanding the
evolution of cooperative behaviour, formally introduced in~\cite{Flood1958}.
Each player has two options, to cooperate (C) or to defect (D). The decisions
are made simultaneously and independently. The normal form representation of the
game is given by:

\begin{equation}\label{equ:pd_definition}
    S_p =
    \begin{pmatrix}
        R & S  \\
        T & P
    \end{pmatrix}
    \quad
    S_q =
    \begin{pmatrix}
        R & T  \\
        S & P
    \end{pmatrix}
\end{equation}

where \(S_p\) represents the utilities of the row player and \(S_q\) the
utilities of the column player. The payoffs, \((R, P, S, T)\), are constrained
by \(T > R > P > S\) and \(2R > T + S\), and the most common values used in the
literature are \((R, P, S, T) = (3, 1, 0, 5)\)~\cite{Axelrod1981}.
The PD is a one shot game, however, it is commonly studied in a manner where the
history of the interactions matters. The repeated form of the game is called the
Iterated Prisoner's Dilemma (IPD).

Memory-one strategies are a set of IPD strategies that have been
studied thoroughly in the literature~\cite{Nowak1990, Nowak1993}, however, they have gained
most of their attention when a certain subset of memory-one strategies was
introduced in~\cite{Press2012}, the zero-determinants. In~\cite{Stewart2012} it
was stated that ``Press and Dyson have fundamentally changed the viewpoint on
the Prisoner's Dilemma''.
Zero-determinants are a special case of memory-one and extortionate
strategies. They choose their actions so that a linear relationship is forced
between the players' score ensuring that they will always
receive at least as much as their opponents. Zero-determinants are
indeed mathematically unique and are proven to be robust in pairwise
interactions, however, their true effectiveness in tournaments and
evolutionary dynamics has been questioned~\cite{adami2013, Hilbe2013b,
Hilbe2013, Hilbe2015, Knight2018, Harper2015}.

In a similar fashion to~\cite{Press2012} the purpose of this work is to consider
a given memory-one strategy; however, whilst~\cite{Press2012} found a way for a
player to manipulate a given opponent, this work will consider a
multidimensional optimisation approach to identify the best response to a given
group of opponents. In particular, this work presents a compact method of
identifying the best response memory-one strategy against a given set of
opponents, and evaluates whether it behaves extortionately, similar to
zero-determinants. This is also done for evolutionary settings. Moreover, we
introduce a well designed framework that allows the comparison of an optimal
memory one strategy and a more complex strategy which has a larger memory and an
identification of conditions for which defection is known to be stable; thus
identifying environments where cooperation will not occur.

\section*{Methods and Results}
\subsection*{Utility}

One specific advantage of memory-one strategies is their mathematical
tractability. They can be represented completely as an element of \(\R^{4}_{[0, 1]}\). This
originates from~\cite{Nowak1989} where it is stated that if a strategy is
concerned with only the outcome of a single turn then there are four possible
`states' the strategy could be in; both players cooperated (\(CC\)), 
the first player cooperated whilst the second player defected (\(CD\)),
the first player defected whilst the second player cooperated (\(DC\)) and
both players defected (\(DD\)).
Therefore, a memory-one strategy can be denoted by the probability vector of
cooperating after each of these states; \(p=(p_1, p_2, p_3, p_4) \in \R_{[0,1]}
^ 4\).

In~\cite{Nowak1989} it was shown that it is not necessary to simulate the play
of a strategy $p$ against a memory-one opponent $q$. Rather this exact behaviour
can be modeled as a stochastic process, and more specifically as a Markov chain
whose corresponding transition matrix \(M\) is
given by (\ref{eq:transition_matrix}). The long run steady state probability
vector \(v\), which is the solution to \(v M = v\), can be
combined with the payoff matrices of (\ref{equ:pd_definition}) to give the expected
payoffs for each player. More specifically, the utility for a memory-one
strategy \(p\) against an opponent \(q\), denoted as \(u_q(p)\), is given by
(\ref{eq:press_dyson_utility}).

\begin{equation}\label{eq:transition_matrix}
    \resizebox{.9\hsize}{!}{$\input{m_matrix.tex}$}
\end{equation}


\begin{equation}\label{eq:press_dyson_utility}
    u_q(p) = v \cdot (R, S, T, P).
\end{equation}

This manuscript has explored the form of \(u_q(p)\), to the authors knowledge no
previous work has done this, and it proves that \(u_q(p)\) is given by a ratio
of two quadratic forms~\cite{kepner2011},
(Theorem~\ref{theorem_one}):

\begin{equation}\label{eq:optimisation_quadratic}
       u_q(p) = \frac{\frac{1}{2}pQp^T + cp + a}
                   {\frac{1}{2}p\bar{Q}p^T + \bar{c}p + \bar{a}},
\end{equation}

where \(Q=Q(q), \bar{Q}=\bar{Q}(q)\) \(\in \R^{4\times4}\), \(c=c(q) \text{ and } \bar{c}=\bar{c}(q)\)
\(\in \R^{4 \times 1}\), \(a=a(q) \text{ and } \bar{a}=\bar{a}(q) \in \R.\)

This can be extended to consider multiple
opponents. The IPD is commonly studied in tournaments and/or Moran Processes
where a strategy interacts with a number of opponents. The payoff of a player in
such interactions is given by the average payoff the player received against
each opponent. More specifically the expected utility of a memory-one strategy
against a \(N\) number of opponents is given by:

\begin{align}\label{eq:tournament_utility}
       & \frac{1}{N} \sum\limits_{i=1} ^ {N} {u_q}^{(i)} (p) = \frac{1}{N} \\
       &
       \frac{\sum\limits_{i=1} ^ {N} (\frac{1}{2} pQ^{(i)} p^T + c^{(i)} p + a^ {(i)})
       \prod\limits_{\tiny\begin{array}{l} j=1 \\ j \neq i \end{array}} ^
       N (\frac{1}{2} p\bar{Q}^{(j)} p^T + \bar{c}^{(j)} p + \bar{a}^ {(j)})}
       {\prod\limits_{i=1} ^ N (\frac{1}{2} p\bar{Q}^{(i)} p^T + \bar{c}^{(i)} p + \bar{a}^ {(i)})}.
\end{align}

Estimating the utility of a memory-one strategy against any number of opponents
without simulating the interactions is the main result used in this manuscript.
It will be used to define best response memory-one strategies, in tournaments
and evolutionary dynamics, and to explore the conditions under which defection
dominates cooperation.

\subsection*{Stability of defection}\label{subsection:stability_defection}

An immediate result from our formulation can be
obtained by evaluating the sign of the utility's (\ref{eq:tournament_utility}) derivative
at \(p=(0, 0, 0, 0)\). If at that point the
derivative is negative, then the utility of a player only decreases if they were
to change their behaviour, and thus \textbf{defection at that point is stable}.

\begin{lemma}\label{lemma:stability_of_defection}
    In a tournament of \(N\) players \(\{q^{(1)}, q^{(2)}, \dots, q^{(N)} \}\)
    for \(q^{(i)} \in \R_{[0, 1]} ^ 4\)
    defection is stable if the transition probabilities of the
    opponents satisfy conditions (\ref{eq:defection_condition_one}) and (\ref{eq:defection_condition_two}).

    \begin{equation}\label{eq:defection_condition_one}
        \sum_{i=1} ^ N (c^{(i)T} \bar{a}^{(i)} - \bar{c}^{(i)T} a^{(i)}) \leq 0
    \end{equation}

    while,

    \begin{equation}\label{eq:defection_condition_two}
        \sum_{i=1} ^ N \bar{a}^{(i)} \neq 0
    \end{equation}
\end{lemma}

\begin{proof}
    For defection to be stable the derivative of the utility
    at the point \(p = (0, 0, 0, 0)\) must be negative. This would indicate that
    the utility function is only declining from that point onwards.

    Substituting \(p = (0, 0, 0, 0)\) in
    equation~(\ref{eq:mo_tournament_derivative}) gives:

    \begin{equation}
    \sum_{i=1} ^ N \frac{(c^{(i)T} \bar{a}^{(i)} - \bar{c}^{(i)T} a^{(i)})}
    {(\bar{a}^{(i)})^2}
    \end{equation}

    The sign of the numerator \( \displaystyle\sum_{i=1} ^ N (c^{(i)T} \bar{a}^{(i)} - \bar{c}^{(i)T} a^{(i)})\)
    can vary based on the transition probabilities of the opponents.
    The denominator can not be negative, and otherwise is always positive.
    Thus the sign of the derivative is negative if and only if
    \( \displaystyle\sum_{i=1} ^ N (c^{(i)T} \bar{a}^{(i)} - \bar{c}^{(i)T} a^{(i)}) \leq 0\).
\end{proof}

Consider a population for which defection is known to be stable. In that
population all the members will over time adopt the same behaviour; thus in such
population cooperation will never take over. This is demonstrated in
Fig.~\ref{fig:stable_defection} and Fig.~\ref{fig:unstable_defection}.

\begin{figure}
        \centering
        \includegraphics[width=.8\linewidth]{population_defection_takes_over.pdf}
        \caption{For opponents \(q_{1}=(\frac{371}{1250},\frac{4693}{25000},\frac{4037}{50000},\frac{18461}{25000})\),
        $q_{2}=(\frac{48841}{100000},\frac{30587}{50000},\frac{76591}{100000},\frac{25921}{50000})$ and
        $q_{3}=(\frac{22199}{100000},\frac{87073}{100000},\frac{646}{3125},\frac{91861}{100000})$
        conditions (\ref{eq:defection_condition_one}) and
        (\ref{eq:defection_condition_two}) hold and Defector takes over the
        population.}
        \label{fig:stable_defection}
\end{figure}
\begin{figure}
        \centering
        \includegraphics[width=.8\linewidth]{population_defection_fails.pdf}
        \caption{For opponents $q_{1}=(\frac{69773}{100000},\frac{21609}{100000},\frac{97627}{100000},\frac{623}{100000})$,
        $q_{2}=(\frac{12649}{50000},\frac{43479}{100000},\frac{38969}{50000},\frac{19769}{100000})$ and
        $q_{3}=(\frac{96703}{100000},\frac{54723}{100000},\frac{24317}{25000},\frac{35741}{50000})$
        (\ref{eq:defection_condition_one}) fails and
        (\ref{eq:defection_condition_two}) holds and Defector does not take over
        the population.}
        \label{fig:unstable_defection}
\end{figure}

\subsection*{Best response memory-one strategies}

As briefly discussed zero-determinants have been acclaimed for their robustness
against a single opponent. Zero-determinants are evidence that extortion works
in pairwise interactions, their behaviour ensures that the strategies will never
lose a game. However, this paper argues that in multi opponent interactions,
where the payoffs matter, strategies trying to exploit their opponents will
suffer.
Compared to zero-determinants, best response memory-one strategies which have a
theory of mind of their opponents, utilise their behaviour in order to gain the
most from their interactions. The question that arises then is whether best
response strategies are optimal because they behave in an extortionate way.

To answer this very question, we initially define \textit{memory-one best response}
strategies as a multi dimensional optimisation problem given by:

\begin{equation}\label{eq:mo_tournament_optimisation}
    \begin{aligned}
    \max_p: & \ \sum_{i=1} ^ {N} {u_q}^{(i)} (p)
    \\
    \text{such that}: & \ p \in \R_{[0, 1]}
    \end{aligned}
\end{equation}

A \textit{best response} is a strategy which corresponds to the most favorable
outcome~\cite{Tadelis2013}, thus a memory-one best response to a set of
opponents \(q^{(1)}, q^{(2)}, \dots, q^{(N)}\) corresponds to a strategy \(p^*\)
for which (\ref{eq:tournament_utility}) is maximised.

Optimising this particular ratio of quadratic forms is not trivial. It can be
verified empirically for the case of a single opponent that there exists at
least one point for which the definition of concavity does not hold.
The non concavity of \(u(p)\) indicates multiple local
optimal points. This is also intuitive. The best response against a cooperator,
\(q=(1, 1, 1, 1)\), is a defector \(p^*=(0, 0, 0, 0)\). The strategies
\(p=(\frac{1}{2}, 0, 0, 0)\) and \(p=(\frac{1}{2}, 0, 0, \frac{1}{2})\) are also
best responses. The approach taken here is to introduce a compact way of
constructing the candidate set of all local optimal points, and evaluating which
corresponds to the best response strategy. The approach is given in Theorem~\ref{memone_group_best_response}.

Finding best response memory-one strategies is analytically feasible using the
formulation of Theorem~\ref{memone_group_best_response} and resultant
theory~\cite{Jonsson2005}. However, for large systems building the resultant
quickly becomes intractable. As a result, best responses will be estimated
heuristically using a numerical method taking advantage of the structure called
Bayesian optimisation~\cite{Mokus1978}.

This is extended to evolutionary settings. In these settings
self interactions are key. Self interactions can be incorporated in the
formulation that has been used so far. More specifically, the optimisation
problem of (\ref{eq:mo_tournament_optimisation}) is extended to include self
interactions:

\begin{equation}\label{eq:mo_evolutionary_optimisation}
\begin{aligned}
\max_p: & \ \frac{1}{N} \sum\limits_{i=1} ^ {N} {u_q}^{(i)} (p) + u_p(p)
\\
\text{such that}: & \ p \in \R_{[0, 1]}
\end{aligned}
\end{equation}

For determining the memory-one best response in an evolutionary setting, an
algorithmic approach is considered, called \textit{best response dynamics}. The
best response dynamics approach used in this manuscript is given by
Algorithm~\ref{algo:best_response_dynamics}.

\begin{algorithm}
       $p^{(t)}\leftarrow (1, 1, 1, 1)$\;
       \While{$p^{(t)} \neq p ^{(t -1)}$}{
       $p^{(t + 1)} =  \text{argmax} \frac{1}{N} \sum\limits_{i=1} ^ {N} {u_q}^{(i)}
       (p^{(t + 1)}) + u_p^{(t)}(p^{(t + 1)})$\;
       }
       \caption{Best response dynamics Algorithm}
       \label{algo:best_response_dynamics}
\end{algorithm}

The results of this section rely on estimating best response memory-one strategies.
Bayesian optimisation is used to generated a data set of best response
memory-one strategies, in tournaments and evolutionary dynamics whilst \(N=2\).
The data set is available at~\cite{glynatsi2019}. It contains a total of 1000 trials
corresponding to 1000 different instances of a best response strategy in
tournaments and evolutionary dynamics. For each trial a set of 2 opponents is
randomly generated and the memory-one best responses against them is found.

The source code used in this manuscript has been written in a sustainable manner.
It is open source (\url{https://github.com/Nikoleta-v3/Memory-size-in-the-prisoners-dilemma})
and tested which ensures the validity of the results. It has also been archived
and can be found at.

In order to investigate whether best responses
behave in an extortionate matter the SSE method as described
in~\cite{Knight2019} is used. The SSE is defined as how far a strategy is from
behaving as a ZD and thus a highand a high SSE implies a non extortionate
behaviour. The SSE method has been applied to the data set. The distribution of
SSE for the best response is given in A statistics summary of the SSE
distribution for the best response in tournaments and evolutionary dynamics is
given in Table~\ref{table:sserror_stats}.

For the best response in tournaments the distribution of SSE is skewed to the
left, indicating that the best response does exhibit extortionate behaviour,
however, the best response is not uniformly extortionate. A positive measure of
skewness and kurtosis indicates a heavy tail to the right. Therefore, in several
cases the strategy is not trying to extort its the opponents. Similarly the
evolutionary best response strategy does not behave uniformly extortionately. A
larger value of both the kurtosis and the skewness of the SSE distribution
indicates that in evolutionary settings a memory-one best response is even more
adaptable.

The difference between best responses in tournaments and in evolutionary
settings are further explored by Fig.~\ref{fig:behaviour_violin_plots}.
Though, no statistically significant differences has been found, from
Fig.~\ref{fig:behaviour_violin_plots}, it seems that evolutionary best
response has a higher $p_2$ median. Thus, they more likely to forgive after
being tricked. This is due to the fact that they could be playing against
themselves, and they need to be able to forgive so that future cooperation can
occur.

\begin{table}
\begin{center}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lrrrrrrrrrrr}
    \toprule
    & mean & std  & 5\% & 50\% &  95\% & max & median & skew & kurt\\
    \midrule
\textbf{tournament} & 0.34  & 0.40  & 0.028  & 0.17  &
1.05  & 2.47  & 0.17  & 1.87 & 3.60 \\
\textbf{evolutionary setting} & 0.17 & 0.23 & 0.01 &
0.12 & 0.67 & 1.53 & 0.12 & 3.42 & 1.92 \\
    \bottomrule
\end{tabular}}
\end{center}
\caption{SSE of best response memory-one when \(N=2\)}\label{table:sserror_stats}
\end{table}

\begin{figure}[!htbp]
    \includegraphics[width=.55\textwidth]{behaviour_violin_plots.pdf}
    \caption{Distributions of \(p^*\) for both best response and evo memory-one
    strategies.}
    \label{fig:behaviour_violin_plots}
\end{figure}

\subsection*{Longer memory best response}

This section focuses on the memory size of strategies. The effectiveness of
memory in the IPD has been previously explored in the literature, however, none
of the works mentioned here have compared the performance of longer-memory
strategies to memory-one best responses.

In~\cite{Harper2017}, a strategy called \textit{Gambler} which makes
probabilistic decisions based on the opponent's \(n_1\) first moves, the
opponent's \(m_1\) last moves and the player's \(m_2\) last moves was
introduced. In this manuscript Gambler with parameters: $n_1 = 2, m_1 = 1$ and $m_2 = 1$ is used
as a longer-memory strategy.
By considering the opponent's first two moves, the opponents last move and the
player's last move, there are only 16 $(4 \times 2 \times 2)$ possible outcomes
that can occur, furthermore, Gambler also makes a probabilistic decision of
cooperating in the opening move. Thus, Gambler is a function \(f: \{\text{C,
D}\} \rightarrow [0, 1]_{\R}\). This can be hard coded as an element
of \([0, 1]_{\R} ^ {16 + 1}\), one probability for each outcome plus the opening
move. Hence, compared to (\ref{eq:mo_tournament_optimisation}), finding an
optimal Gambler is a 17 dimensional problem given by:

\begin{equation}\label{eq:gambler_optimisation}
    \begin{aligned}
    \max_p: & \ \sum_{i=1} ^ {N} {U_q}^{(i)} (f)
    \\
    \text{such that}: & \ f \in \R_{[0, 1]}^{17}
    \end{aligned}
\end{equation}

Note that (\ref{eq:tournament_utility}) can not be used here for the utility
of Gambler, and actual simulated players are used. This is done using~\cite{axelrodproject}
with 500 turns and 200 repetitions, moreover, (\ref{eq:gambler_optimisation})
is solved numerically using Bayesian optimisation.

Similarly to previous sections, a large data set has been generated with
instances of an optimal Gambler and a memory-one best response, available
at~\cite{glynatsi2019}. Estimating a best response Gambler (17 dimensions) is
computational more expensive compared to a best response memory-one (4
dimensions). As a result, the analysis of this section is based on a total of
130 trials. For each trial two random opponents have been selected. The 130 pair
of opponents are a sub set of the opponents used in the previous section.

The utilities of both strategies are plotted against each other in
Fig.~\ref{fig:utilities_gambler_mem_one}.
It is evident from Fig.~\ref{fig:utilities_gambler_mem_one} that
Gambler always performs as well as the best response memory-one or better. This seems to be at odd with the
result of~\cite{Press2012} that against a memory-one opponent having a longer memory
will not give a strategy any
advantage. However, against two memory-one opponents, Gambler's performance is better than
the optimal memory-one strategy. This is evidence that in the case of two opponents having a
shorter memory is limiting and this is potentially another example of the advantages
of adaptability.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=.45\textwidth]{gambler_performance_against_mem_one.pdf}
    \caption{Utilities of Gambler and best response memory-one strategies for
    130 different pair of opponents.}\label{fig:utilities_gambler_mem_one}
\end{figure}

\section*{Discussion}

This manuscript has considered \textit{best response} strategies in the IPD game, and
more specifically, \textit{memory-one best responses}. It has proven that there is
a compact way of identifying a memory-one best response to a group of opponents,
and moreover, that there exists a condition for which in an
environment of memory-one opponents defection is the stable choice.
The later parts of this paper focused on a series of empirical results, where it
was shown that the performance and the evolutionary stability of memory-one
strategies rely not on extortion but on adaptability. Finally, it was shown that
memory-one strategies' performance is limited by their memory in cases where
they interact with multiple opponents.

Following the work described in~\cite{Nowak1989}, where it was shown that the
utility between two memory-one strategies can be estimated by a Markov
stationary state, we proved that the utilities can be written as a ration of two
quadratic forms in $R^4$, Theorem~\ref{theorem_one}. This was extended to
include multiple opponents, as the IPD is commonly studied in such situations.
This formulation allowed us to introduce an approach for identifying memory-one
best responses to any number of opponents;
Theorem~\ref{memone_group_best_response}. This does not only have game theoretic
novelty, but also a mathematical novelty of solving quadratic ratio optimisation
problem where the quadratics are non concave. The results of were also used to
define a condition for which defection is known to be stable.

This manuscript presented several experimental results. All data for the results
is archived in~\cite{glynatsi2019}. These results were mainly to investigate the
behaviour of memory-one strategies and their limitations. A large data set which
contained best responses in tournaments and in evolutionary settings for $N=2$
was generated. This allowed us to investigate their respective behaviours, and
whether it was extortionate acts that made them the most favorable strategies.
However, it was shown that it was not extortion but adaptability that allowed
the strategies to gain the most from their interactions. In evolutionary
settings it was specifically shown that being adaptable and being able to
forgive after being tricked were key factors. Moreover, the performance of
memory-one strategies was put against the performance of a longer memory
strategy called Gambler. There were several cases where Gambler would outperform
the memory-one strategy, however, a memory-one strategy never managed to
outperform a Gambler. This result occurred whilst considering a Gambler with a
sufficiently larger memory but not a sufficiently larger amount of information
regarding the game.

All the empirical results presented in this manuscript have been for the case of
$N=2$. In future work we would consider larger values of $N$, however, we
believe that for larger values of $N$ the results that have been presented here
would only be more evident. In addition, we would investigate potential
theoretical results for the evolutionary best responses dynamics algorithm
discussed.

\showmatmethods{} % Display the Materials and Methods section

\section*{Appendix}

\begin{theorem}\label{theorem_one}

    The expected utility of a memory-one strategy \(p\in\mathbb{R}_{[0,1]}^4\)
    against a memory-one opponent \(q\in\mathbb{R}_{[0,1]}^4\), denoted
    as \(u_q(p)\), can be written as a ratio of two quadratic forms:

    \begin{equation}\label{eq:optimisation_quadratic}
    u_q(p) = \frac{\frac{1}{2}pQp^T + cp + a}
                {\frac{1}{2}p\bar{Q}p^T + \bar{c}p + \bar{a}},
    \end{equation}
    where \(Q, \bar{Q}\) \(\in \R^{4\times4}\) are square matrices defined by the
    transition probabilities of the opponent \(q_1, q_2, q_3, q_4\) as follows:

    \begin{center}
    \begin{equation}
    \resizebox{.95\linewidth}{!}{\arraycolsep=2.5pt%
    \boldmath\(
    Q = \input{q_numerator}\)},
    \end{equation}
    \begin{equation}\label{eq:q_bar_matrix}
    \resizebox{.9\linewidth}{!}{\arraycolsep=2.5pt%
    \boldmath\(
    \bar{Q} =  \input{q_denominator}\)}.
    \end{equation}
    \end{center}

    \(c \text{ and } \bar{c}\) \(\in \R^{4 \times 1}\) are similarly defined by:

    \begin{equation}\label{eq:q_matrix_numerator}
    \resizebox{0.5\linewidth}{!}{\arraycolsep=2.5pt%
    \boldmath\(c = \input{c_numerator}\),}
    \end{equation}
    \begin{equation}\label{eq:q_matrix_denominator}
    \resizebox{0.5\linewidth}{!}{\arraycolsep=2.5pt%
    \boldmath\(\bar{c} = \input{c_denominator}\),
    }
    \end{equation}
    and the constant terms \(a, \bar{a}\) are defined as \(a = \input{numerator_constant}\) and
    \(\bar{a} = \input{denominator_constant}\).
\end{theorem}

Proof is given in (SI).

\begin{theorem}\label{memone_group_best_response}

The optimal behaviour of a memory-one strategy player
\(p^* \in \R_{[0, 1]} ^ 4\)
against a set of \(N\) opponents \(\{q^{(1)}, q^{(2)}, \dots, q^{(N)} \}\)
for \(q^{(i)} \in \R_{[0, 1]} ^ 4\) is given by:

\[p^* = \textnormal{argmax}\sum\limits_{i=1} ^ N  u_q(p), \ p \in S_q.\]

The set \(S_q\) is defined as all the possible combinations of:

{\tiny
\begin{equation}\label{eq:s_q_set}
    S_q =
    \left\{p \in \mathbb{R} ^ 4 \left|
        \begin{aligned}
            \bullet\quad p_j \in \{0, 1\} & \quad \text{and} \quad \frac{d}{dp_k} 
            \sum\limits_{i=1} ^ N  u_q^{(i)}(p) = 0 \\
            & \quad \text{for all} \quad j \in J \quad \&  \quad k \in K  \quad \text{for all} \quad J, K \\
            & \quad \text{where} \quad J \cap K = \O \quad
            \text{and} \quad J \cup K = \{1, 2, 3, 4\}.\\
            \bullet\quad  p \in \{0, 1\} ^ 4
        \end{aligned}\right.
    \right\}.
\end{equation}
}

Note that there is no immediate way to find the zeros of
\(\frac{d}{dp} \sum\limits_{i=1} ^ N  u_q(p)\) where,

{\tiny
\begin{align}\label{eq:mo_tournament_derivative}
    \frac{d}{dp} \sum\limits_{i=1} ^ {N} {u_q}^{(i)} (p) & = \nonumber \\
    & = \displaystyle\sum\limits_{i=1} ^ {N} & 
    \frac{\left(pQ^{(i)} + c^{(i)}\right) \left(\frac{1}{2} p\bar{Q}^{(i)} p^T + \bar{c}^{(i)} p + \bar{a}^ {(i)}\right)}
    {\left(\frac{1}{2} p\bar{Q}^{(i)} p^T + \bar{c}^{(i)} p + \bar{a}^ {(i)}\right)^ 2} \nonumber \\ 
     & & - \frac{\left(p\bar{Q}^{(i)} + \bar{c}^{(i)}\right) \left(\frac{1}{2} pQ^{(i)} p^T + c^{(i)} p + a^ {(i)}\right)}
    {\left(\frac{1}{2} p\bar{Q}^{(i)} p^T + \bar{c}^{(i)} p + \bar{a}^ {(i)}\right)^ 2}
\end{align}
}

For \(\frac{d}{dp} \sum\limits_{i=1} ^ N  u_q(p)\) to equal zero then:

{\scriptsize
\begin{align}\label{eq:polynomials_roots}
    \displaystyle\sum\limits_{i=1} ^ {N}
    & \left(pQ^{(i)} + c^{(i)}\right) \left(\frac{1}{2} p\bar{Q}^{(i)} p^T + \bar{c}^{(i)} p + \bar{a}^ {(i)}\right) \\
    & - \left(p\bar{Q}^{(i)} + \bar{c}^{(i)}\right) \left(\frac{1}{2} pQ^{(i)} p^T + c^{(i)} p + a^ {(i)}\right)
    &= 0, \quad {while} \\
    \displaystyle\sum\limits_{i=1} ^ {N} & \frac{1}{2} p\bar{Q}^{(i)} p^T + \bar{c}^{(i)} p + \bar{a}^ {(i)} \neq 0.
\end{align}}

\end{theorem}

Proof is given in (SI).

\acknow{
A variety of software libraries have been used in this work,
the Axelrod library for IPD simulations~\cite{axelrodproject},
the Scikit-optimize library for an implementation of Bayesian optimisation~\cite{tim_head_2018_1207017},
the Matplotlib library for visualisation~\cite{hunter2007matplotlib},
the SymPy library for symbolic mathematics~\cite{sympy} and
the Numpy library for data manipulation~\cite{walt2011numpy}.
}

\showacknow{} % Display the acknowledgments section

% Bibliography
\bibliography{pnas-sample}

\end{document}
